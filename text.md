---
title: Platform Regulation
subtitle: a civil society proposal
---

Dominant Internet platforms like Facebook, Amazon and Google are more and more becoming the arena of social and legal conflicts. We witness a worldwide debate about potential new rules for dominant social media platforms (a so called new "platform regulation"). These debates are highly complex in a law-based society because they require us to resolve the conflict between fundamental rights and risk delegation of essential tasks to private actors. Yet, the negative effects of harmful behaviour by these actors increases political appetite for regulation.

To navigate the upcoming debate, we want to propose and collect concrete policy solutions and evaluate them within the fundamental rights framework of the European Union. The project aims at a broader acceptance of developed positions within European civil society. The first step is to anchor the positions found through this investigative process in the expert discussion on digital rights and with organisations in the fields of human rights and consumer protection. The probability that this area will be newly regulated in the next EU legislative period (2019-2024) is very high. Should the EU Commission initiate a legislative proposal before there is an established position on fundamental rights, the chance for poor EU regulations will increase (see Copyright Directive). Therefore, it is necessary to develop this position together now.

This is a policy proposal in the form and in the spirit of a request for comments. We invite everybody to participate in the discussion, provide feedback, and propose amendments on any of the proposals outlined below on this website. [feedback@platformregulation.eu](mailto:feedback@platformregulation.eu)

# Definitions and basic concepts

## `MUST` Types of recommendations
`MUST` This word means that the definition is an absolute requirement of the specification.

`MUST NOT` This phrase means that the definition is an absolute prohibition of the specification.

`RECOMMENDED` This word means that there may exist valid reasons in particular circumstances to ignore a particular item, but the full implications must be understood and carefully weighed before choosing a different course.

`NOT RECOMMENDED` This phrase means that there may exist valid reasons in particular circumstances when the particular behaviour is acceptable or even useful, but the full implications should be understood and the case carefully weighed before implementing any behaviour described with this label.

`DISCUSS` Policy proposal that is worth discussion within the community and requires further evaluation.

> [RFC2119 | Key words for use in RFCs to Indicate Requirement Levels](https://tools.ietf.org/html/rfc2119/)

## `MUST` Scope Limitations
These policy recommendations and discussions are limited in their scope to democratic countries with a stable rule of law and strong fundamental right protections.

## `MUST` Online platforms
By online platforms, we indicate a service that provides an intermediary function in the access to information, goods or services that are residing on the systems or networks at the direction of users.

> Definition from [Conseil national du numérique, Ambition numérique, Pour une politique française européenne de la transition numérique](https://cnnumerique.fr/files/2017-10/CNNum--rapport-ambition-numerique.pdf) and the DMCA.

## `MUST` Social media platforms
By social media platforms, we understand those online platforms that operate with user-generated content and curate the contents of their users through algorithmic or editorial decisions.

> See also: [EU | Digital Single Market | Online Platforms](https://ec.europa.eu/digital-single-market/en/online-platforms-digital-single-market/), [Even Under Kind Masters: A Proposal to Require that Dominant Platforms Accord Their Users Due Process](https://www.publicknowledge.org/assets/uploads/blog/Even_Under_Kind_Masters.pdf#page=9) p.8 and [Online platforms and how to regulate them: an EU overview](https://www.bertelsmann-stiftung.de/fileadmin/files/user_upload/EZ_JDI_OnlinePlatforms_Dittrich_2018_ENG.pdf#page=4) p.4

## `MUST` Relevant platforms
By dominant platforms, we understand online or social media platforms that have significant market power in a country in the EEA and a minimum global revenue of 50 Million Euros.

> Examples of definitions for significant market power and how a regulator should assess them can be found in telecommunication law, e.g. [Article 35 auf the Austrian Telecom Code](https://www.rtr.at/en/tk/TKG2003/Telecommunications_Act_2003__unofficial_.pdf)

## `MUST` Dominant platforms
By dominant platforms, we understand online or social media platforms that have significant market power in a majority of countries in the EEA and a minimum global revenue of 500 Million Euros.

## `MUST` Sponsored Content
By sponsored content, we understand all content that has been paid for to be displayed or enlarge its reach.

## `MUST` Dark Content
By dark content, we understand all sponsored content that is not published on the account of the advertiser, but displayed only to a specified target audience.

> [What is the Difference Between a Sponsored Post and a Paid Ad?](https://blog.envisionitsolutions.com/what-is-the-difference-between-a-sponsored-post-and-a-paid-ad)

## `MUST` API Accessibility
By API accessible, we understand a computer information system that gives access to all content via a unique identifier. All Data needs to be downloadable in bulk, by day, week, year and per country. New data needs to be accessible via the system within a day of being published. APIs should be designed in a way to sustain independent research and long-term studies.

> [Facebook and Google: This is What an Effective Ad Archive API Looks Like](https://blog.mozilla.org/blog/2019/03/27/facebook-and-google-this-is-what-an-effective-ad-archive-api-looks-like/)
> [Facebook's Ad Archive API is Inadequate](https://blog.mozilla.org/blog/2019/04/29/facebooks-ad-archive-api-is-inadequate/)

## `MUST` Public Decentralised Ledger
TODO

## `MUST` Political Accounts
By political accounts, we understand accounts run by or acting on behalf of political parties, associations affiliated with political parties or politically exposed persons as defined by Article 2 of EU Directive 2006/70/EC.

> [EU Definition PEP](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32015L0849)

## `MUST` Peters Problem
The problem of algorithmically generated user profiles that diverge significantly from the attributes and preferences of the underlying human being.
> Marc-Uwe Kling: [QualityLand](https://qualityland.de/) (dunkle Edition). Ullstein, Berlin 2017, ISBN 3550050151.

# Content Regulation

## `MUST` Procedural Safeguards for Content Notifications
**A central pitfall of the current notice and action regime is the lack of procedural safeguards for the notification procedure.** Every online platform needs to present to the user easily accessible, user-friendly and contextual notification options. These options should be available without the obligation to sign-in or sign-up with the service itself, if the content in question is publicly available.

1. Notifications need to offer a category of different types of violations ranging from various classes of illegal content to legal content that might be in breach with Terms of Services or other rules of the platform. Different notification categories trigger different procedures, which take into account the fundamental rights of all parties in question. Therefore, procedures with higher safeguards can’t be substituted with lower ones. For example, a notice of illegality with the possibility of legal redress can’t be circumvented by deletion of the content in question under the terms of services of the platform.

2. A valid notice should be sufficiently precise and adequately substantiated. This should include 1) the location of the content; 2) the reason for the complaint (potentially including legal basis under which the content has to be assessed); 3) evidence of the claim and potentially legal standing; 4) a declaration of good faith that the information provided is accurate and potentially 5) considerations on limitations, exceptions, and defences available to the content provider. Only in notifications of copyright infringements is the identification information of the notifier mandatory. In all other cases identification and contact information of the notifier are optional.

3. For purposes of procedural fairness and increasing the quality of content-moderation, the content provider should be informed about a notification of his or her content, the reason for the notification, information about the subsequent process and possible ways to appeal or file a counter-notice. The content provider should be informed immediately once the platform has received the notification and not just after a decision has been taken. Exceptions from this obligation to notify the content provider might apply only if sending notifications would hamper ongoing law enforcement investigations.

4. Possibility for counter notifications should be offered to the content-provider to respond to the claim of the original notifier with evidence and arguments to the contrary. This counter-notification should be an option even before a decision by the platform is taken. Both original notification and counter-notification should apply the same standards in terms of declarations of good faith and possible punishment for misrepresentation of facts.

5. Intermediaries have to inform the parties involved in notification about the outcome of the decision an intermediary has take in their case. This communication is always send to content providers and to notifiers if they have provided contact details in their notice. This communication needs to include 1) the reasoning of the platform for why it came to this decision; 2) the circumstances via which the decision was made, and if the decision was made by a human or algorithmic decision agent; and 3) information about the potential to appeal this decision by either party with the intermediary, courts or other entities. This communication should also be send for counter-notices.

6. Online platforms need to publish information about their procedures and time frames for intervention by interested parties. This information should include 1) time until a notification is sent to the content provider; 2) the time for the content provider to respond with a counter notice; 3) the average and maximum time for a decision by the intermediary for categories of cases; 4) the time at which the intermediary will inform both parties about the result of the procedure; 5) the time for different forms of appeal against the decision.

> This proposals stems strongly from [Kuczerawy, Aleksandra, Safeguards for Freedom of Expression in the Era of Online Gatekeeping (September 11, 2018). Auteurs & Media, 2018, Forthcoming.](https://ssrn.com/abstract=3247682) which builds on top of the [Manila Principles](https://www.manilaprinciples.org/).

## `RECOMMENDED` Notice and fair balance for illegal content
**The current notice and takedown system should be replaced by a notice-and-fair-balance procedure, which obligates different types of actions depending on the content that gets notified and the affected fundamental rights.** To allow for transparency and sufficient oversight within the rule of law, the required procedure by a notification of illegal content (notice of illegality) can't be preempted by the deletion of this content based on the Terms of Services or Community Guidelines of the platform. Otherwise, the economic incentive of the dominant Platform would stipulate overblocking potentially legal content to avoid a more burdensome procedure.

1. **efficient notice and take down on dangerous threats and calls for violence**
A comprehensive ruleset on a national level shall define a clear set of cases of dangerous threats and calls for violence against individuals or protected groups (hate speech). There is no general monitoring obligation for the platform, but once a platform obtains a notification of illegality it has to assess it in a timely manner. If the content poses an imminent danger to life or existence, the deadline for the conclusion of the content moderation is shorter. Once knowledge of content in conflict with these rules is established, the dominant Platform needs to take immediate action to temporarily block and report the case to government authorities. The platforms must inform the complainant and the person affected by such deletion of the outcome and justification for the case. Both parties have the possibility to appeal the decision or the process of notification with a competent authority. This authority can overturn the decision of the platform and/or permanently delete blocked content. If the competent authority decides that the decision of the platform regarding the notified content was not justified, or recognizes that a report of a user was not reproachfully pursued, a fine for the online platform must be imposed.
2. **automated deletion of absolutely illegal content**
An automated notice-and-stay-down procedure can never be obliged of a platform and can only be an admissible reaction to content that violates fundamental rights, which must not be assessed by the principles of proportionality (e.g. child pornography, beheading videos). Only if such content can be detected by automated systems with a very high probability (99,9%), then the system can be used in practice. The technical systems used for automated deletion must follow an open peer-reviewed methodology, be periodically reviewed by an independent body, and require annual transparency reports on the success rate of the systems. The attempted publication of such content constitutes a serious criminal offence and therefore platforms must inform authorities of all such cases. A public authority with judicial oversight has to determine for all individual content whether it meets the requirement of this category. Affected users have the right to a redress mechanism against the automatic deletion of their content.
3. **state examination of contentious or non-severe content**
For content that does not fall under the aforementioned offences, there must be a government body that can swiftly issue injunctions to clarify whether the platform operator is required to delete them. Platforms need to notify state authorities and must be given a certain deadline to handle cases.

> [ Angelopoulos, Christina and Smet, Stijn, Notice-and-Fair-Balance: How to Reach a Compromise between Fundamental Rights in European Intermediary Liability (October 21, 2016). This is an Authors' Original Version of an article published by Taylor & Francis in Journal of Media Law (2016) on 21 Oct 2016, DOI: 10.1080/17577632.2016.1240957.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2944917)

## `MUST` Social media council
TODO
> For best practices on Conflict of Interest policies see the [Commission Decision C(2016)3301/F1](http://ec.europa.eu/transparency/regdoc/?fuseaction=list&coteId=3&year=2016&number=3301&version=ALL&language=en) on establishing [expert groups](http://ec.europa.eu/transparency/regexpert/index.cfm?do=faq.faq&aide=2)

## `DISCUSS` Deliberative social media council
**A European multi-stakeholder body that creates guidelines and minimum quality standards for content moderation.** It oversees the implementation of these guidelines by the platforms operating in European countries and can be called upon by users or the platform to issue opinions on particular cases or groups of cases that follow a common theme. All decisions and recommendations of the body are non-binding.

> [A Human Rights Approach to Platform Content Regulation](https://freedex.org/a-human-rights-approach-to-platform-content-regulation/)

## `DISCUSS` Social media appeals council
**An (inter)national multi-stakeholder arbitration body that oversees the content moderation of social media platforms.** Users can appeal content moderation decisions of the platform to this body, to ensure independent review and oversight. (Appeals concerning content, disputable due to illegality are not to be reviewed by this council, see policy recommendation 2.1 under notice and fair-balance.) The Board must be comprised of a diverse group of experts, representing a range of different views and experiences. Members of the Board should be selected in a transparent and independent manner. The Board should be in charge of the selection of cases, and the procedures for bringing cases and obtaining evidence should be set out in detail. The Board must have a clear and public code of ethics and must be fully independent and able to make genuine, impartial and recognized decisions. Its decisions have to be based on a Charta which is fully compliant with internationally recognized human rights law.

> [Facebook oversight board: Recommendations for human rights-focused oversight](https://www.article19.org/resources/facebook-oversight-board-recommendations-for-human-rights-focused-oversight/)

> [Draft Charter: An Oversight Board for Content Decisions](https://fbnewsroomus.files.wordpress.com/2019/01/draft-charter-oversight-board-for-content-decisions-2.pdf)

## `RECOMMENDED` Enforcement via European Platform Regulator
**A competent regulatory authority in the form of a European Agency is tasked with the enforcement and supervision of the obligations for online platforms.** The regulatory authority is tasked to ensure the compliance with rules on notice and action procedures, reporting and information requirements, advertisement transparency, service inter-operability and the cooperation of the platform based on law with other competent authorities. Furthermore, the regulator has to ex-ante approve the Terms of Service of any dominant social media platform (including other documents relevant to content-moderation and account suspension, like Community Guidelines and Code of Conducts). It is explicitly outside of the scope of the supervision and enforcement duties of the regulator to make any decisions about the legality or permissibility of individual content or classes of content. The regulator can impose penalties of up to 2% of annual global revenue. The organization shall follow the Common Approach of the European Union and has to publish bi-annual reports on all of its activities to fulfill its mandate.

## `MUST` Establish registered office to interact with legal system
Dominant platforms have to establish registered offices in the EU countries where they conduct their business. These offices allow local law enforcement and courts to reach the platform under their jurisdiction.

> Wikipedia: [Ladungsfähige Adresse](https://de.wikipedia.org/wiki/Ladungsf%C3%A4hige_Anschrift) and [registered office](https://en.wikipedia.org/wiki/Registered_office)

## `MUST` Update interfacing between law enforcement and platforms
TODO

## `MUST` Update the legal system
The legal system and law enforcement is currently ill-equipped to deal with the increasing amount of cases that involve an internet related element. Investments in general training and capacity building throughout, as well as the establishment of specialised units, would enable law enforcement to meet the demands of an information society.

## `MUST` Clarifying Terms of Service
Terms of Service (TOS) should be written in a clear and understandable language and should contain examples of acceptable and unacceptable behaviour for every rule on content moderation.

## `RECOMMENDED` Trusted Flaggers
**Dominant social media platforms may appoint trusted flaggers within a country.** Notifications of trusted flaggers are dealt with more expeditiously than others, but they are subject to the same safeguards as regulator notifications. A list of all current and previous trusted flaggers has to be published by the platform.

## `RECOMMENDED` Prevent real name policy by suspending accounts
Whereas a real name policy is an unproportionate measure that can easily be circumvented, dominant social media platforms shall instead suspend accounts that can't be connected to a real person once a court has ordered them to do so. The suspension has to be time limited and can be restricted to users accessing the platform from the country where the court order has been issued. The user behind the account has to be notified about the proceedings by the platform and can file a counter-notice to be taken into account in the considerations of the court.

> [original idea](https://background.tagesspiegel.de/statt-klarnamen-digitales-gewaltschutzgesetz)

# Algorithmic accountability and disinformation

## `MUST` Empower users to take control over algorithmic curation of informations
**Users must have an easily accessible option to sort the content being displayed to them** Dominant social media platforms have to offer this possibility to users. The setting should incorporate at least a fully chronological timeline, but also other factors which empower the user to take control over their information diet. Users can take these decisions persistently over the duration of individual sessions. The concrete options the platform have to offer can be evaluated by the regulator, which can issue guidance on potential additions and the design of the feature. This obligation does not exclude the potential insertion of sponsored content.

> This improves the user's understanding of algorithms, by seeing how much content is otherwise hidden from them. It also enables them to understand the amount of content posted by accounts they follow. Technically this option should not create an undue burden for the platform provider.

## `DISCUSS` Liability for discriminatory recommendation algorithms
*** After a negatively discriminatory effect of algorithmic recommendations of dominant platforms has been proven, and after obtaining knowledge that the platform sustains that same negative effect for a prolonged period, the platform becomes liable for the damages caused to the infringed group.*** Associations of marginalised groups can bring class-action cases to court to establish the facts of the case and subsequently ask for damages. The decision about the discriminating effect have to be established by court with the possibility of both parties to appeal.

> Currently platforms face no responsibility for potential negative effects by their algorithmic recommendations. If those negative effects can be demonstrated without a reasonable doubt and persist even after being established, the platform needs to face a penalty for the damages they cause to society. This proposal still allows for ample room for innovation and only applies to established dominant platforms. Liability can be avoided by changes to the functionality of the service once the discrimination has been established.

## `MUST` Scientific access to dominant platforms via committee safeguard
**Establishment of an EU committee which receives and decides on research applications from independent academic institutions.** The approval is dependent on ethical data protection and scientific standards of the research proposal. Once approved, the dominant social media platform has to grant access to the defined data sets. An oversight board enforces the compliance of researchers and the platform with the agreed data protection and research standards. Data provided by the dominant platform needs to be consistent and in a standardized machine-readable format.

> [Social Science One](https://socialscience.one/): positive attempt that got stuck because of failed sanctions for refusal of dominant platforms to cooperate.

## `DISCUSS` Scientific access to dominant platforms with differential privacy safeguard
**Dominant platforms should provide access to their data via a differential privacy interface to the researchers selected by a committee.** To protect private user data, the differential privacy measure introduces statistical noise into the output of every query.

> Different systems of randomization of user data could be bypassed by requesting multiple sets of data to reverse engineer the randomization process. Thereby the risk of large amounts of personal data being published is higher than the benefits possibly gained by publically accessible research data.

> Possible options could be providing quarterly sets of randomized data for public research, which is only once randomized and then published as such.

> [Nahles, Daten für alle Gesetzesvorschlag](https://www.zdf.de/nachrichten/heute/nahles-will-inernet-riesen-grenzen-aufzeigen-100.html)

## `MUST` Transparency Reports
**Proportionate transparency obligations have to empower users to adequately assess the trustworthiness of platforms.** Reporting obligations have to be fulfilled with a proportionate regularity and in an openly licensed, easily understandable and machine-readable format. Platforms are required to publish such reports proportionate to their size, market share and the potential risk for users.
Transparency reports need to be published on the following topics:

1. **Report on law-enforcement information requests on user data** containing, at least, on the total number of requests for user data and the total number of accounts affected, the sensitivity of the data requested, the total number of fully complied with requests and separately the total number of requests with which the platform has not or only partially complied. This data must be provided per country, per legal basis for the request and if different security authorities are involved, also per authority.

> [Current **Twitter** transparency on legal information request](https://transparency.twitter.com/en/information-requests.html)

> [Current **Google** transparency on legal information requests](https://transparencyreport.google.com/user-data/overview)

> [Current **Facebook** transparency on legal information requests](https://transparency.facebook.com/government-data-requests)

> [Current **Microsoft** transparency on legal information requests](https://www.microsoft.com/en-us/corporate-responsibility/lerr)

> [Current **Apple** transparency on legal information requests](https://www.apple.com/legal/transparency/)

2. **Report on legal requests on content and account blocking** containing at least the total global number of legal requests for content to be blocked and separately for content to be deleted, the total number of accounts affected by the request, the total number of requests complied with and separately the total number of requests complied with partially or not at all. The total number of requests for blocking accounts, the total number of accounts affected thereby and the type of legal demands requiring for content to be blocked or deleted should also be included. This data must be provided per country, per legal basis and, if different security authorities are involved, also per authority.

> [Current **Facebook** transparency on legal content restrictions](https://transparency.facebook.com/content-restrictions)

> [Current **Twitter** transparency on legal content removal](https://transparency.twitter.com/en/removal-requests.html)

3. **Report on the enforcement of terms of services** containing at least the total number of account blockings or suspensions and content deletion in the categories of violations. The reporting needs to include the average time between publication of the content, notification, potential counter-notification and action categorized by the different sections of the terms of service, actions that where taken and if the decisions where partially or in full automated. Dominant platforms need to lay out how the enforcement of the terms of services is implemented and overseen. The report should also highlight all cases where the outcome of a content moderation decision based on terms of services contradicted the outcome of a notification of illegality.

> [Current **Twitter** transparency on terms of service enforcement](https://transparency.twitter.com/en/twitter-rules-enforcement.html)

> [Current **Facebook** transparency on community standards enforcement](https://transparency.facebook.com/community-standards-enforcement)

> [The Santa Clara Principles](https://santaclaraprinciples.org/)

## `RECOMMENDED` Rectification of behavioural profiles (Peters Problem)
**Users must be enabled to rectify and edit their personal advertisement profile.** The user can have information changed that has arisen from algorithms due to incorrect data, as well as information that an algorithm has incorrectly composed from correct information without the necessity to prove the truthfulness of the request. The user-interface of the platform needs to display the option for rectification close to every targeted advertisement that is based on profiling.

> Aspects of this proposal are already covered by the [Right to rectification | Art. 16 GDPR](https://gdpr-info.eu/art-16-gdpr/). However, algorithmic assumptions about a person that can't be disputed with facts are not covered by the right to rectification.
> [Current Facebook Ad Preference Screen](https://www.facebook.com/ads/preferences/?entry_product=ad_settings_screen)

## `MUST` Advertisement Archive
**Dominant platforms need to publish sponsored content either displayed within the European Union or paid for by an account registered within the European Union in a public advertisement archive.** This archive must contain all sponsored content displayed within the last 7 years, with full functionality as they were displayed to the user. The additional information stored in this archive must be also provided in a machine-readable format and accessible by an API. Additional information that needs to be supplied within the archive includes: whether the sponsored content is currently active or inactive; the start date for active content and the timespan in which the sponsored content was active for inactive content; the name of the advertiser; the total number of impressions; the exact description of the target group; the exact amount of money paid and, while active, the estimated amount. For sponsored content that needs to be depublished due to terms of service violations or legal proceedings, the additional information needs to stay in the ad-archive and further information about the type of rule violation or pending lawsuits needs to be provided. Each piece of sponsored content must contain an attached info button that directly links to the content within the Advertisement Archive.

> This provides more transparency about commercial advertisement in general and, by building awareness, this may also have a positive impact on public manipulation in general.
> In addition to algorithmic transparency, the possibility to understand the reason that an advertisement is shown to you may also be an important step in understanding why people see what they see online (algorithmic content composition).
> For more information on the current Ad Libraries see Description on "Political Advertisement Archive".
> [About Facebook Ads](https://www.facebook.com/ads/about/?entry_product=ad_library)

## `RECOMMENDED` Rectifying disinformation and defamatory content
**The rectification or apology for content on dominant social media platforms that has been ruled as disinformation or defamation by a court needs to be published by the platform on channels with equivalent audiences to the original content.** Once a court has ordered the content provider to issue a rectification or apology statement according to national media or civil law, the obligation of the content provider shall extend to the dominant social media platform to publish this statement on the same and with the same parameters via which the original content was displayed to users. The purpose of this obligation is to reach the same or an equivalent audience. To implement this obligation, the platform is not obliged to track user behaviour or retain additional information about user interactions.

> See [NETPEACE](https://netpeace.eu): "Right to digital counter statement: notification of rectification in case of identified false reports due to court decisions. In the field of false reports / honorary offences, the right to a digital counter statement should be established or expanded, according to which all those users who have been notified of a judicially established hoax or defamation must also be notified of the counter-notification. Any notifications of rectification must be sent out via all channels in which the hoax was displayed (i.e. also in the profiles of those users who shared the hoax) and to all users who interacted with the causing message (likes, comments) Etc.). The obligation to correct is to be designed in a way that there is no obligation for additional tracking."

## `MUST` Prohibition of Dark Content
**Dark content provided by political accounts is generally prohibited.** All content that is published by a public political accounts needs to be visible on the account page.

> Personalised election promises are fostering misinformation and weaken democratic discourse. Every political message needs to be accountable and subject to public scrutiny.  

## `MUST` Political Advertisement Archive
**All political sponsored content needs to be centrally visible in a public advertisement archive.** This archive must store all political sponsored content for 30 years. This archive must contain all sponsored content displayed within the last 7 years, with their full functionality as displayed to the user. The additional information stored in this archive must be also provided in a machine-readable format and accessible by an API. Additional information that needs to be supplied within the archive includes: whether the sponsored content is currently active or inactive; the start date for active content and the timespan in which the sponsored content was active for inactive content; the name of the advertiser; the total number of impressions; the exact description of the target group; the exact amount of money paid and, while active, the estimated amount. According to a follow-the-money approach, intermediaries have to list the ultimate client or beneficiary of the sponsored content. (Political sponsored content must be distinguishable from common sponsored content. To differentiate political sponsored content from common content, political accounts need to register with the platform and subsequently be distinguishable from common accounts.) To increase accountability of political actors, politically sponsored content needs to contain a link referencing this content in the political advertisement archive. For sponsored content that needs to be depublished due to terms of service violations or legal proceedings, the additional information needs to stay in the ad-archive and further information about the type of rule violation or pending lawsuits needs to be provided. Each piece of sponsored content must contain an attached info button that directly links to the content within the advertisement Archive.

> political accounts:
[Facebook on ads related to politics or issues of national importance](https://www.facebook.com/business/help/214754279118974?helpref=faq_content)
[Facebook authorization process for political accounts](https://www.facebook.com/business/help/208949576550051)
[Facebook getting started for political accounts](https://www.facebook.com/gms_hub/share/getting-started-with-facebook-ads_english-us-.pdf)


> Political ads are not restricted to political parties, leaders, or foundations. Therefore, it is important to create a general advertisement archive. See: Advertisement Archive.

> It is important to list the ultimate beneficiary because political propaganda may be also be spread by dummy accounts to bypass regulation concerning political advertisement.

> Providing the exact amount of money spent on a political online advertisement is also a requirement to effectively monitor political campaign regulation.
[EU- country comparison on political campaign regulation](https://rm.coe.int/use-of-internet-in-electoral-campaigns-/16807c0e24#page=13) p. 12

> [Current **Facebook** political Ad Library](https://www.facebook.com/ads/library/?active_status=all&ad_type=political_and_issue_ads&country=ALL)

> [Current **Twitter** political Ad Library](https://ads.twitter.com/transparency/i/political_advertisers)

> [Current **Google** general political Ad Library](https://transparencyreport.google.com/political-ads/home)

> [Current **Google** EU political Ad Library](https://transparencyreport.google.com/political-ads/region/EU)

> [Bing banning the political advertisement](https://about.ads.microsoft.com/en-us/blog/post/october-2018/changes-to-our-political-ads-policy)

> [Instagram currently only provides the option to Report political ads within the US and display such ads within Facebooks Ad Library](https://help.instagram.com/118613625676963)

> [What is Facebook doing to secure elections](https://m.facebook.com/help/1991443604424859)

# Data portability and interoperability

## `MUST` Interoperability and Competition
**Dominant platforms have to publish technical standards developed for data interoperability and data portability under an open licence.**

> This is to enable startups and small businesses to grow and profit from user data collected by existing platforms. Incumbents often profit from the network effect of their existing user base. Fostering data portability and inter-operability drives innovation and freedom of choice for users.

> See [Data transfer project](https://datatransferproject.dev/), [Data transfer project whitepaper](https://datatransferproject.dev/dtp-overview.pdf), [Right to data portability | Art. 20 GDPR](https://gdpr-info.eu/art-20-gdpr/), [Economics of open and closed systems - switching costs p. 8](http://www.autoritedelaconcurrence.fr/doc/economics_open_closed_systems.pdf) and [Tim Berners-Lee | Solid | true data ownership](https://solid.mit.edu/)

## `MUST` Privacy and security over data portability
**Data that is exported from any service to be transferred to another service must not contain any private information of third parties without their consent.** Procedures for data portability must respect user privacy of third parties. Special functionality should be in place to ask for the consent of third parties to have their data included in the export of that belonging to another user. This would enable the movement of parts of the social network of a user to another platform.

## `RECOMMENDED` Restrict unauthorized access
To reduce the risk of unauthorized access to summarized, structured user data within a transfer functionality, different security measures need to be implemented.

1. Two-Factor Authentication
To secure particularly sensitive functionalities, a second factor authentication of the user should be required. Potential implementations include authenticator APPs, hardware tokens or SMS verification.

2. Rate Limiting & user notification
Platforms further must inform the user on failed as well as successful attempts to export data with a data transfer service. Furthermore, limiting the rate users can attempt to transfer data can reduce the impact of compromised accounts.

> [Rate Limiting - datatransferproject whitepaper](https://datatransferproject.dev/dtp-overview.pdf#page=15)

## `RECOMMENDED` Stricter merger control
**Similar to the telecommunication market, mergers between and acquisitions of dominant platforms are subject to approval by competition authorities.** The public authority shall take into account the effect of the merger on consumer choice, on the potential concentration of market power, on risks of gatekeeping roles in other markets, as well as on the concentration of user data under one centralized entity. Additionally, the “potential competition” test should be applied more consistently to prevent that bigger firms absorb small companies like start-ups that in the future could become competitors. Authorities can prohibit or allow the merger and also place conditions on it.

> Partly based on [BEUC: Shaping Competition Policy in the era of digitalisation](https://www.beuc.eu/publications/beuc-x-2018-084_beuc_response_shaping_of_competition_policy.pdf). See also [It's Time to Break Up Facebook - NYT](https://www.nytimes.com/2019/05/09/opinion/sunday/chris-hughes-facebook-zuckerberg.html)

## `RECOMMENDED` Reparation of the harm caused to consumers and competition
**An effective collective enforcement mechanism empowers consumers to obtain timely redress in case of competition law infringements.** While consumers are the ones ultimately affected by abusive conduct, they currently have little or no remedies at their disposal. Therefore, the scope of the Directive on Representative Actions for the protection of the collective interests of consumers should also include infringements of competition law. Further to this, part of the fines imposed on companies for breaches of competition law should contribute to projects and initiatives aiming at creating a culture of compliance and helping consumers to reap the benefits of competitive markets.
> Based on [BEUC: Shaping Competition Policy in the era of digitalisation](https://www.beuc.eu/publications/beuc-x-2018-084_beuc_response_shaping_of_competition_policy.pdf).

## `DISCUSS` Effective assessment of market power in digital markets
The criteria upon which market power is assessed should include proxies, such as the control of data necessary for the creation and provisions of services. Abuses of competition power often also entail other breaches, such as consumer law or privacy protections. Close cooperation between competent authorities is a key requirement for effective enforcement.
